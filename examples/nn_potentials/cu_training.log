
===============================================================================

                           _   ___      _
                          | | /   |    | |
                 _ __ ___ | |/ /| | ___| |__   ___ _ __ ___
                | '_ ` _ \| / /_| |/ __| '_ \ / _ \ '_ ` _ \
                | | | | | | \___  | (__| | | |  __/ | | | | |
                |_| |_| |_|_|   |_/\___|_| |_|\___|_| |_| |_|


ML4Chem is Machine Learning for Chemistry and Materials. This package is
written in Python 3, and intends to offer modern and rich features to perform
machine learning workflows for chemical physics.

This project is directed by Muammar El Khatib.


Contributors (in alphabetic order):
-----------------------------------
    Elijah Gardella     : Interatomic potentials for ionic systems.
    Jacklyn Gee         : Gaussian features class improvements, and cjson
                          reader.

===============================================================================


Data
====
Module accessed on 2020-03-07 12:55:28.
Data structure is not compatible with ML4Chem.
Preparing images for training...
Images hashed and processed...

There are 40 atoms in your data set.
 
Featurization
=============
Module accessed on 2020-03-07 12:55:28.
Getting unique element symbols for training
Unique chemical elements: ['Cu']
Making default symmetry functions...
Number of features per chemical element:
    - Cu: 8.
 
Symmetry function parameters for Cu atom:
-----------------------------------------
  #      Symbol    Type Parameters
  0   Cu            G2  eta: 0.0500
  1   Cu            G2  eta: 0.2321
  2   Cu            G2  eta: 1.0772
  3   Cu            G2  eta: 5.0000
  4   Cu, Cu        G3  eta: 0.0050 gamma:  1.0000 zeta: 1.0000
  5   Cu, Cu        G3  eta: 0.0050 gamma: -1.0000 zeta: 1.0000
  6   Cu, Cu        G3  eta: 0.0050 gamma:  1.0000 zeta: 4.0000
  7   Cu, Cu        G3  eta: 0.0050 gamma: -1.0000 zeta: 4.0000


Data preprocessing
------------------
Preprocessor: MinMaxScaler.
Options:
    - feature_range: (-1, 1).
 

Embarrassingly parallel computation of atomic features...
... finished in 0 hours 0 minutes 1.30 seconds.

Converting features to dask array...
Shape of array is (40, 8) and chunks ((4, 4, 4, 4, 4, 4, 4, 4, 4, 4), (8,)).
Calling feature preprocessor...
Stacking features using atoms index map...
Featurization finished in 0 hours 0 minutes 7.75 seconds.
features saved to features.db.
 
Model
=====
Module accessed on 2020-03-07 12:55:38.
Model name: PytorchPotentials.
Number of hidden-layers: 2
Structure of Neural Net: (input, 10, 10, output)
Total number of parameters: 213.
Number of training parameters: 213.
 
ModuleDict(
  (Cu): Sequential(
    (0): Linear(in_features=8, out_features=10, bias=True)
    (1): ReLU()
    (2): Linear(in_features=10, out_features=10, bias=True)
    (3): ReLU()
    (4): Linear(in_features=10, out_features=1, bias=True)
  )
)
Initialization of weights with Xavier Uniform by default.

Training
========
Convergence criteria: {'energy': 0.005}
Loss function: AtomicMSELoss


Batch Information
-----------------
Number of batches: 1.
Batch size: 10 elements per batch.
 
Optimizer
---------
Name: Adam.
Options:
    - lr: 0.01.
    - weight_decay: 0.0.
 
 
Starting training...

Epoch  Time Stamp          Loss         Error/img    Error/atom
------ ------------------- ------------ ------------ ------------
     1 2020-03-07 12:55:38 6.878925e-04 4.691754e-02 1.172939e-02
     2 2020-03-07 12:55:38 3.355091e-04 3.276628e-02 8.191571e-03
     3 2020-03-07 12:55:38 1.498477e-04 2.189777e-02 5.474444e-03
     4 2020-03-07 12:55:38 2.838064e-04 3.013603e-02 7.534008e-03
     5 2020-03-07 12:55:38 2.006465e-04 2.533908e-02 6.334769e-03
     6 2020-03-07 12:55:38 2.120516e-05 8.237507e-03 2.059377e-03
     7 2020-03-07 12:55:38 6.062964e-05 1.392892e-02 3.482230e-03
     8 2020-03-07 12:55:39 1.698979e-04 2.331680e-02 5.829201e-03
     9 2020-03-07 12:55:39 1.649551e-04 2.297512e-02 5.743781e-03
    10 2020-03-07 12:55:39 8.424114e-05 1.641864e-02 4.104659e-03
    11 2020-03-07 12:55:39 3.902505e-05 1.117498e-02 2.793745e-03
    12 2020-03-07 12:55:39 5.037100e-05 1.269595e-02 3.173988e-03
    13 2020-03-07 12:55:39 5.824228e-05 1.365193e-02 3.412984e-03
    14 2020-03-07 12:55:39 5.172614e-05 1.286560e-02 3.216400e-03
    15 2020-03-07 12:55:39 6.306174e-05 1.420555e-02 3.551387e-03
    16 2020-03-07 12:55:39 6.723042e-05 1.466756e-02 3.666890e-03
    17 2020-03-07 12:55:39 3.601413e-05 1.073523e-02 2.683808e-03
    18 2020-03-07 12:55:39 7.797351e-06 4.995150e-03 1.248788e-03
Training finished in 0 hours 0 minutes 1.37 seconds.
