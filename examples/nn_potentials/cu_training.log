
===============================================================================

                           _   ___      _
                          | | /   |    | |
                 _ __ ___ | |/ /| | ___| |__   ___ _ __ ___
                | '_ ` _ \| / /_| |/ __| '_ \ / _ \ '_ ` _ \
                | | | | | | \___  | (__| | | |  __/ | | | | |
                |_| |_| |_|_|   |_/\___|_| |_|\___|_| |_| |_|


ML4Chem is Machine Learning for Chemistry and Materials. This package is
written in Python 3, and intends to offer modern and rich features to perform
machine learning workflows for chemical physics.

This project is directed by Muammar El Khatib.


Contributors (in alphabetic order):
-----------------------------------
    Elijah Gardella     : Interatomic potentials for ionic systems.
    Jacklyn Gee         : Gaussian features class improvements, and cjson
                          reader.

===============================================================================

Data
====
Module accessed on 2020-02-11 10:21:43.
Data structure is not compatible with ML4Chem.
Preparing images for training...
Images hashed and processed...

There are 40 atoms in your data set.
 
Featurization
=============
Module accessed on 2020-02-11 10:21:43.
Getting unique element symbols for training
Unique chemical elements: ['Cu']
Making default symmetry functions...
Number of features per chemical element:
    - Cu: 8.
 
Symmetry function parameters for Cu atom:
-----------------------------------------
  #      Symbol    Type Parameters
  0   Cu            G2  eta: 0.0500
  1   Cu            G2  eta: 0.2321
  2   Cu            G2  eta: 1.0772
  3   Cu            G2  eta: 5.0000
  4   Cu, Cu        G3  eta: 0.0050 gamma:  1.0000 zeta: 1.0000
  5   Cu, Cu        G3  eta: 0.0050 gamma: -1.0000 zeta: 1.0000
  6   Cu, Cu        G3  eta: 0.0050 gamma:  1.0000 zeta: 4.0000
  7   Cu, Cu        G3  eta: 0.0050 gamma: -1.0000 zeta: 4.0000

Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.
Data preprocessing
------------------
Preprocessor: MinMaxScaler.
Options:
    - feature_range: (-1, 1).
 

Embarrassingly parallel computation of atomic features...
... finished in 0 hours 0 minutes 1.04 seconds.

Converting features to dask array...
Shape of array is (40, 8) and chunks ((4, 4, 4, 4, 4, 4, 4, 4, 4, 4), (8,)).
Calling feature preprocessor...
Stacking features using atoms index map...
Featurization finished in 0 hours 0 minutes 18.45 seconds.
features saved to features.db.
 
Model
=====
Module accessed on 2020-02-11 10:22:04.
Model name: PytorchPotentials.
Number of hidden-layers: 2
Structure of Neural Net: (input, 10, 10, output)
Total number of parameters: 213.
Number of training parameters: 213.
 
ModuleDict(
  (Cu): Sequential(
    (0): Linear(in_features=8, out_features=10, bias=True)
    (1): ReLU()
    (2): Linear(in_features=10, out_features=10, bias=True)
    (3): ReLU()
    (4): Linear(in_features=10, out_features=1, bias=True)
  )
)
Initialization of weights with Xavier Uniform by default.
 
Batch Information
-----------------
Number of batches: 1.
Batch size: 10 elements per batch.
 
Optimizer
---------
Name: Adam.
Options:
    - lr: 0.01.
    - weight_decay: 0.0.
 
 
Starting training...
 
Epoch  Time Stamp          Loss         RMSE/img RMSE/atom
------ ------------------- ------------ -------- ---------
     1 2020-02-11 10:22:04 2.565196e-03 0.090601 0.022650
     2 2020-02-11 10:22:04 2.524519e-03 0.089880 0.022470
     3 2020-02-11 10:22:04 1.826923e-03 0.076460 0.019115
     4 2020-02-11 10:22:04 1.449829e-03 0.068114 0.017028
     5 2020-02-11 10:22:04 1.374900e-03 0.066330 0.016583
     6 2020-02-11 10:22:04 9.654508e-04 0.055583 0.013896
     7 2020-02-11 10:22:04 4.068858e-04 0.036084 0.009021
     8 2020-02-11 10:22:05 8.929012e-05 0.016904 0.004226
     9 2020-02-11 10:22:05 1.394901e-04 0.021127 0.005282
    10 2020-02-11 10:22:05 3.645262e-04 0.034154 0.008538
    11 2020-02-11 10:22:05 4.392713e-04 0.037492 0.009373
    12 2020-02-11 10:22:05 3.304328e-04 0.032517 0.008129
    13 2020-02-11 10:22:05 1.544800e-04 0.022234 0.005558
    14 2020-02-11 10:22:05 3.899370e-05 0.011170 0.002793
    15 2020-02-11 10:22:05 4.817205e-05 0.012416 0.003104
    16 2020-02-11 10:22:05 1.417603e-04 0.021299 0.005325
    17 2020-02-11 10:22:05 2.187349e-04 0.026457 0.006614
    18 2020-02-11 10:22:05 2.266819e-04 0.026933 0.006733
    19 2020-02-11 10:22:05 1.877701e-04 0.024513 0.006128
    20 2020-02-11 10:22:05 1.405975e-04 0.021211 0.005303
    21 2020-02-11 10:22:05 9.500597e-05 0.017436 0.004359
    22 2020-02-11 10:22:05 4.792237e-05 0.012384 0.003096
    23 2020-02-11 10:22:05 2.184977e-05 0.008362 0.002090
    24 2020-02-11 10:22:05 4.185869e-05 0.011574 0.002893
    25 2020-02-11 10:22:06 8.458525e-05 0.016452 0.004113
    26 2020-02-11 10:22:06 1.001379e-04 0.017901 0.004475
    27 2020-02-11 10:22:06 7.885323e-05 0.015885 0.003971
    28 2020-02-11 10:22:06 4.519446e-05 0.012026 0.003006
    29 2020-02-11 10:22:06 2.476861e-05 0.008903 0.002226
    30 2020-02-11 10:22:06 1.822594e-05 0.007637 0.001909
    31 2020-02-11 10:22:06 2.696426e-05 0.009289 0.002322
    32 2020-02-11 10:22:06 4.729608e-05 0.012302 0.003076
    33 2020-02-11 10:22:06 5.611506e-05 0.013400 0.003350
    34 2020-02-11 10:22:06 4.393630e-05 0.011857 0.002964
    35 2020-02-11 10:22:06 2.739472e-05 0.009363 0.002341
    36 2020-02-11 10:22:06 1.776021e-05 0.007539 0.001885
    37 2020-02-11 10:22:06 1.168118e-05 0.006114 0.001528
    38 2020-02-11 10:22:06 1.256086e-05 0.006340 0.001585
    39 2020-02-11 10:22:06 2.283545e-05 0.008548 0.002137
    40 2020-02-11 10:22:06 2.847632e-05 0.009546 0.002386
    41 2020-02-11 10:22:07 2.153377e-05 0.008301 0.002075
    42 2020-02-11 10:22:07 1.239791e-05 0.006299 0.001575
    43 2020-02-11 10:22:07 8.344489e-06 0.005167 0.001292
    44 2020-02-11 10:22:07 7.103641e-06 0.004768 0.001192
Training finished in 0 hours 0 minutes 2.92 seconds.
