
===============================================================================

          ███╗   ███╗██╗██╗  ██╗ ██████╗██╗  ██╗███████╗███╗   ███╗
          ████╗ ████║██║██║  ██║██╔════╝██║  ██║██╔════╝████╗ ████║
          ██╔████╔██║██║███████║██║     ███████║█████╗  ██╔████╔██║
          ██║╚██╔╝██║██║╚════██║██║     ██╔══██║██╔══╝  ██║╚██╔╝██║
          ██║ ╚═╝ ██║███████╗██║╚██████╗██║  ██║███████╗██║ ╚═╝ ██║
          ╚═╝     ╚═╝╚══════╝╚═╝ ╚═════╝╚═╝  ╚═╝╚══════╝╚═╝     ╚═╝



ML4Chem is Machine Learning for Chemistry. This package is written in Python
3, and intends to offer modern and rich features to perform machine learning
workflows for chemical physics.

This project is directed by Muammar El Khatib.


Contributors (in alphabetic order):
-----------------------------------
    Elijah Gardella     : Interatomic potentials for ionic systems.
    Jacklyn Gee         : Gaussian features class improvements, and cjson
                          reader.

===============================================================================

Data
====
Data structure is not compatible with ML4Chem.
Preparing images for training...
Images hashed and processed...

There are 40 atoms in your data set.
 
Fingerprinting
==============
Getting unique element symbols for training
Unique chemical elements: ['Cu']
Making default symmetry functions...
Number of features per chemical element:
    - Cu: 8.
 
Symmetry function parameters for Cu atom:
-----------------------------------------
  #      Symbol    Type Parameters
  0   Cu            G2  eta: 0.0500
  1   Cu            G2  eta: 0.2321
  2   Cu            G2  eta: 1.0772
  3   Cu            G2  eta: 5.0000
  4   Cu, Cu        G3  eta: 0.0050 gamma:  1.0000 zeta: 1.0000
  5   Cu, Cu        G3  eta: 0.0050 gamma: -1.0000 zeta: 1.0000
  6   Cu, Cu        G3  eta: 0.0050 gamma:  1.0000 zeta: 4.0000
  7   Cu, Cu        G3  eta: 0.0050 gamma: -1.0000 zeta: 4.0000

Data preprocessing
------------------
Preprocessor: MinMaxScaler.
Options:
    - feature_range: (-1, 1).
 

Adding atomic feature calculations to scheduler...
... finished in 0 hours 0 minutes 0.89 seconds.

Computing features...
Fingerprinting finished in 0 hours 0 minutes 12.33 seconds.
features saved to features.db.
 
Model Training
==============
Model name: PytorchPotentials.
Number of hidden-layers: 2
Structure of Neural Net: (input, 10, 10, output)
Total number of parameters: 213.
Number of training parameters: 213.
 
ModuleDict(
  (Cu): Sequential(
    (0): Linear(in_features=8, out_features=10, bias=True)
    (1): ReLU()
    (2): Linear(in_features=10, out_features=10, bias=True)
    (3): ReLU()
    (4): Linear(in_features=10, out_features=1, bias=True)
  )
)
Initialization of weights with Xavier Uniform by default.
 
Batch Information
-----------------
Number of batches: 1.
Batch size: 10 elements per batch.
 
Optimizer
---------
Name: Adam.
Options:
    - lr: 0.01.
    - weight_decay: 0.0.
 
 
Starting training...
 
Epoch  Time Stamp          Loss         RMSE/img RMSE/atom
------ ------------------- ------------ -------- ---------
     1 2019-08-02 10:57:53 2.667184e-03 0.092385 0.023096
     2 2019-08-02 10:57:53 1.960350e-03 0.079203 0.019801
     3 2019-08-02 10:57:53 2.110164e-03 0.082174 0.020543
     4 2019-08-02 10:57:53 1.857089e-03 0.077089 0.019272
     5 2019-08-02 10:57:53 1.397502e-03 0.066873 0.016718
     6 2019-08-02 10:57:53 1.028824e-03 0.057378 0.014345
     7 2019-08-02 10:57:53 7.837244e-04 0.050079 0.012520
     8 2019-08-02 10:57:53 5.278861e-04 0.041100 0.010275
     9 2019-08-02 10:57:53 2.563576e-04 0.028642 0.007160
    10 2019-08-02 10:57:53 7.797596e-05 0.015796 0.003949
    11 2019-08-02 10:57:53 1.251908e-04 0.020015 0.005004
    12 2019-08-02 10:57:53 3.607562e-04 0.033977 0.008494
    13 2019-08-02 10:57:53 5.033503e-04 0.040134 0.010033
    14 2019-08-02 10:57:53 4.306156e-04 0.037121 0.009280
    15 2019-08-02 10:57:53 2.674354e-04 0.029254 0.007313
    16 2019-08-02 10:57:53 1.417568e-04 0.021298 0.005325
    17 2019-08-02 10:57:53 8.169171e-05 0.016168 0.004042
    18 2019-08-02 10:57:54 6.599159e-05 0.014532 0.003633
    19 2019-08-02 10:57:54 7.927317e-05 0.015927 0.003982
    20 2019-08-02 10:57:54 1.147875e-04 0.019166 0.004791
    21 2019-08-02 10:57:54 1.584415e-04 0.022517 0.005629
    22 2019-08-02 10:57:54 1.892787e-04 0.024611 0.006153
    23 2019-08-02 10:57:54 1.908680e-04 0.024714 0.006178
    24 2019-08-02 10:57:54 1.604215e-04 0.022657 0.005664
    25 2019-08-02 10:57:54 1.109057e-04 0.018839 0.004710
    26 2019-08-02 10:57:54 6.487968e-05 0.014409 0.003602
    27 2019-08-02 10:57:54 4.047438e-05 0.011381 0.002845
    28 2019-08-02 10:57:54 3.885511e-05 0.011151 0.002788
    29 2019-08-02 10:57:54 4.636037e-05 0.012180 0.003045
    30 2019-08-02 10:57:54 5.454547e-05 0.013212 0.003303
    31 2019-08-02 10:57:54 6.703910e-05 0.014647 0.003662
    32 2019-08-02 10:57:54 8.024852e-05 0.016025 0.004006
    33 2019-08-02 10:57:54 7.901930e-05 0.015902 0.003975
    34 2019-08-02 10:57:54 5.916118e-05 0.013759 0.003440
    35 2019-08-02 10:57:55 3.588460e-05 0.010716 0.002679
    36 2019-08-02 10:57:55 2.487515e-05 0.008922 0.002230
    37 2019-08-02 10:57:55 2.567280e-05 0.009064 0.002266
    38 2019-08-02 10:57:55 2.889367e-05 0.009616 0.002404
    39 2019-08-02 10:57:55 3.105708e-05 0.009969 0.002492
    40 2019-08-02 10:57:55 3.469847e-05 0.010537 0.002634
    41 2019-08-02 10:57:55 3.918731e-05 0.011198 0.002800
    42 2019-08-02 10:57:55 3.922247e-05 0.011203 0.002801
    43 2019-08-02 10:57:55 3.196334e-05 0.010113 0.002528
    44 2019-08-02 10:57:55 2.215865e-05 0.008421 0.002105
    45 2019-08-02 10:57:55 1.709579e-05 0.007396 0.001849
    46 2019-08-02 10:57:55 1.782907e-05 0.007553 0.001888
    47 2019-08-02 10:57:55 1.950972e-05 0.007901 0.001975
    48 2019-08-02 10:57:55 1.997949e-05 0.007996 0.001999
    49 2019-08-02 10:57:55 2.131088e-05 0.008258 0.002065
    50 2019-08-02 10:57:55 2.306165e-05 0.008591 0.002148
    51 2019-08-02 10:57:55 2.155194e-05 0.008305 0.002076
    52 2019-08-02 10:57:56 1.674580e-05 0.007320 0.001830
    53 2019-08-02 10:57:56 1.308212e-05 0.006470 0.001618
    54 2019-08-02 10:57:56 1.282202e-05 0.006406 0.001601
    55 2019-08-02 10:57:56 1.372179e-05 0.006626 0.001657
    56 2019-08-02 10:57:56 1.381838e-05 0.006650 0.001662
    57 2019-08-02 10:57:56 1.393750e-05 0.006678 0.001670
    58 2019-08-02 10:57:56 1.455957e-05 0.006826 0.001706
    59 2019-08-02 10:57:56 1.423744e-05 0.006750 0.001687
    60 2019-08-02 10:57:56 1.225655e-05 0.006263 0.001566
    61 2019-08-02 10:57:56 1.026859e-05 0.005732 0.001433
    62 2019-08-02 10:57:56 9.836881e-06 0.005611 0.001403
    63 2019-08-02 10:57:56 1.022367e-05 0.005720 0.001430
    64 2019-08-02 10:57:56 1.021854e-05 0.005718 0.001430
    65 2019-08-02 10:57:56 1.014250e-05 0.005697 0.001424
    66 2019-08-02 10:57:56 1.037434e-05 0.005762 0.001440
    67 2019-08-02 10:57:56 1.012194e-05 0.005691 0.001423
    68 2019-08-02 10:57:57 9.044534e-06 0.005380 0.001345
    69 2019-08-02 10:57:57 8.142257e-06 0.005104 0.001276
    70 2019-08-02 10:57:57 8.040231e-06 0.005072 0.001268
    71 2019-08-02 10:57:57 8.137205e-06 0.005103 0.001276
    72 2019-08-02 10:57:57 7.974038e-06 0.005051 0.001263
    73 2019-08-02 10:57:57 7.877318e-06 0.005021 0.001255
    74 2019-08-02 10:57:57 7.856756e-06 0.005014 0.001254
    75 2019-08-02 10:57:57 7.497682e-06 0.004898 0.001225
Training finished in 0 hours 0 minutes 4.57 seconds.
